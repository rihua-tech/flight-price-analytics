{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports & paths\n",
    "\n",
    "Load core libraries, point to the project/data locations, and confirm dependency versions plus file availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX  # optional seasonal ARIMA capability\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    HAVE_PROPHET = True\n",
    "    PROPHET_IMPORT_ERROR = None\n",
    "except Exception as exc:  # pragma: no cover - environment dependent\n",
    "    HAVE_PROPHET = False\n",
    "    PROPHET_IMPORT_ERROR = exc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJ = Path(r\"D:\\MGA\\job\\data analyst\\flight-price-analytics\")\n",
    "DATA = PROJ / \"data\"\n",
    "FACT = DATA / \"fares_fact.csv\"\n",
    "OUT_DETAIL = DATA / \"forecast_detail.csv\"\n",
    "OUT_SUM = DATA / \"forecast_summary.csv\"\n",
    "\n",
    "print(f\"Notebook run at: {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "print(f\"Pandas {pd.__version__} | NumPy {np.__version__}\")\n",
    "print(f\"Prophet available: {HAVE_PROPHET}\")\n",
    "if not HAVE_PROPHET:\n",
    "    print(f\"Prophet import error: {PROPHET_IMPORT_ERROR}\")\n",
    "\n",
    "for path in [PROJ, DATA, FACT]:\n",
    "    print(f\"{path} exists: {path.exists()}\")\n",
    "\n",
    "if FACT.exists():\n",
    "    size_mb = FACT.stat().st_size / 1_048_576\n",
    "    print(f\"Fact file size: {size_mb:,.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load clean fact & prep\n",
    "\n",
    "Read the fare fact table, enforce numeric pricing, drop incomplete rows, create the route key, and roll up to a tidy time-series table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fares_fact(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load fares_fact.csv with date parsing and price coercion.\"\"\"\n",
    "    date_cols = [\"search_date\", \"depart_date\", \"snapshot_date\"]\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=date_cols,\n",
    "        dtype={\"source_name\": \"string\", \"origin\": \"string\", \"destination\": \"string\"},\n",
    "    )\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"price\"] + date_cols)\n",
    "    df[\"origin\"] = df[\"origin\"].str.strip().str.upper()\n",
    "    df[\"destination\"] = df[\"destination\"].str.strip().str.upper()\n",
    "    df[\"route\"] = df[\"origin\"] + \"-\" + df[\"destination\"]\n",
    "    return df\n",
    "\n",
    "def make_timeseries(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate to a single median price per route-date snapshot.\"\"\"\n",
    "    ts = (\n",
    "        df.groupby([\"route\", \"snapshot_date\"], as_index=False)[\"price\"]\n",
    "        .median()\n",
    "        .rename(columns={\"snapshot_date\": \"date\"})\n",
    "        .sort_values([\"route\", \"date\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return ts\n",
    "\n",
    "fact_df = load_fares_fact(FACT)\n",
    "ts = make_timeseries(fact_df)\n",
    "\n",
    "print(f\"Raw fact rows: {len(fact_df):,} | Routes: {fact_df['route'].nunique():,}\")\n",
    "print(f\"Time-series rows: {len(ts):,} spanning {ts['date'].min().date()} to {ts['date'].max().date()}\")\n",
    "print(ts.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Seasonal-naive baseline (weekly p=7)\n",
    "\n",
    "Create both a simple row-shift and a nearest-neighbor seasonal-naive baseline and keep the most informed value for each route-date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_naive_shift(ts_df: pd.DataFrame, lag: int = 7, value_col: str = \"price\", out_col: str = \"baseline_7d\") -> pd.DataFrame:\n",
    "    \"\"\"Shift each route's series by the requested lag to form a fast baseline.\"\"\"\n",
    "    work = ts_df.sort_values([\"route\", \"date\"]).copy()\n",
    "    work[out_col] = work.groupby(\"route\", sort=False)[value_col].shift(lag)\n",
    "    return work\n",
    "\n",
    "def seasonal_naive_nearest(ts_df: pd.DataFrame, lag_days: int = 7, tolerance: str = \"1D\", value_col: str = \"price\", out_col: str = \"baseline_7d_nn\") -> pd.DataFrame:\n",
    "    \"\"\"Use merge_asof to find the closest snapshot near t-7 when exact matches are missing.\"\"\"\n",
    "    work = ts_df.sort_values([\"route\", \"date\"]).copy()\n",
    "    history = work[[\"route\", \"date\", value_col]].copy()\n",
    "    history[\"date\"] = history[\"date\"] + pd.Timedelta(days=lag_days)\n",
    "    history = history.rename(columns={value_col: out_col})\n",
    "    merged = pd.merge_asof(\n",
    "        work,\n",
    "        history,\n",
    "        on=\"date\",\n",
    "        by=\"route\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(tolerance),\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "ts_shift = seasonal_naive_shift(ts)\n",
    "ts_nn = seasonal_naive_nearest(ts_shift)\n",
    "ts_nn[\"fcst_baseline\"] = ts_nn[\"baseline_7d_nn\"].combine_first(ts_nn[\"baseline_7d\"])\n",
    "ts_features = ts_nn.copy()\n",
    "\n",
    "coverage = 1 - ts_features[\"fcst_baseline\"].isna().mean()\n",
    "print(f\"Baseline coverage (rows with forecast): {coverage:.1%}\")\n",
    "print(ts_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Rolling backtest setup (walk-forward)\n",
    "\n",
    "Filter to rows with a valid baseline, align actual vs. forecast chronologically, and add rolling diagnostics for later KPIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_baseline(ts_df: pd.DataFrame, actual_col: str = \"price\", forecast_col: str = \"fcst_baseline\") -> pd.DataFrame:\n",
    "    \"\"\"Retain rows with baseline forecasts for evaluation.\"\"\"\n",
    "    cols = [\"route\", \"date\", actual_col, forecast_col]\n",
    "    bt = ts_df.loc[:, cols].dropna(subset=[forecast_col]).copy()\n",
    "    bt = bt.rename(columns={actual_col: \"actual\", forecast_col: \"fcst_baseline\"})\n",
    "    bt = bt.sort_values([\"route\", \"date\"]).reset_index(drop=True)\n",
    "    return bt\n",
    "\n",
    "def add_backtest_features(bt_df: pd.DataFrame, window: str = \"30D\", min_periods: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"Append rolling medians and win-rate helpers per route.\"\"\"\n",
    "    enriched = []\n",
    "    for route, grp in bt_df.groupby(\"route\", sort=False):\n",
    "        grp = grp.sort_values(\"date\").copy()\n",
    "        grp[\"abs_error\"] = (grp[\"actual\"] - grp[\"fcst_baseline\"]).abs()\n",
    "        median_price = grp.set_index(\"date\")[\"actual\"].rolling(window=window, min_periods=min_periods).median()\n",
    "        grp[\"median_30d\"] = median_price.values\n",
    "        median_abs_error = grp.set_index(\"date\")[\"abs_error\"].rolling(window=window, min_periods=min_periods).median()\n",
    "        grp[\"median_abs_err_30d\"] = median_abs_error.values\n",
    "        grp[\"win_flag\"] = grp[\"abs_error\"] <= grp[\"median_abs_err_30d\"]\n",
    "        grp[\"win_flag\"] = grp[\"win_flag\"].fillna(False)\n",
    "        enriched.append(grp)\n",
    "    return pd.concat(enriched, ignore_index=True)\n",
    "\n",
    "bt_baseline = backtest_baseline(ts_features)\n",
    "bt_with_features = add_backtest_features(bt_baseline)\n",
    "\n",
    "print(f\"Backtest rows: {len(bt_with_features):,}\")\n",
    "print(f\"Routes modeled: {bt_with_features['route'].nunique():,}\")\n",
    "print(bt_with_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) KPIs & win-rate summary\n",
    "\n",
    "Aggregate MAE/MAPE and rolling win-rate per route to understand baseline quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpis_by_route(df: pd.DataFrame, actual_col: str = \"actual\", pred_col: str = \"fcst_baseline\", win_flag_col: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Compute MAE, MAPE, counts, and optional win-rate for each route.\"\"\"\n",
    "    records: list[dict] = []\n",
    "    for route, grp in df.groupby(\"route\", sort=False):\n",
    "        grp_valid = grp.dropna(subset=[actual_col, pred_col])\n",
    "        if grp_valid.empty:\n",
    "            continue\n",
    "        actual = grp_valid[actual_col].astype(float)\n",
    "        pred = grp_valid[pred_col].astype(float)\n",
    "        abs_err = (actual - pred).abs()\n",
    "        mae = abs_err.mean()\n",
    "        mape = np.nan\n",
    "        mape_mask = actual > 0\n",
    "        if mape_mask.any():\n",
    "            mape = (abs_err[mape_mask] / actual[mape_mask]).mean() * 100\n",
    "        record = {\n",
    "            \"route\": route,\n",
    "            \"mae\": mae,\n",
    "            \"mape\": mape,\n",
    "            \"n_obs\": len(grp_valid),\n",
    "            \"start_date\": grp_valid[\"date\"].min(),\n",
    "            \"end_date\": grp_valid[\"date\"].max(),\n",
    "        }\n",
    "        if win_flag_col and win_flag_col in grp_valid.columns:\n",
    "            record[\"win_rate\"] = grp_valid[win_flag_col].mean()\n",
    "        records.append(record)\n",
    "    result = pd.DataFrame(records)\n",
    "    if not result.empty:\n",
    "        result = result.sort_values(\"mape\").reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "baseline_summary = kpis_by_route(bt_with_features, win_flag_col=\"win_flag\")\n",
    "baseline_summary = baseline_summary.rename(\n",
    "    columns={\n",
    "        \"mae\": \"mae_baseline\",\n",
    "        \"mape\": \"mape_baseline\",\n",
    "        \"n_obs\": \"n_obs\",\n",
    "        \"win_rate\": \"win_rate_baseline\",\n",
    "    }\n",
    ")\n",
    "if \"win_rate_baseline\" not in baseline_summary.columns:\n",
    "    baseline_summary[\"win_rate_baseline\"] = np.nan\n",
    "\n",
    "print(f\"Baseline KPI routes: {len(baseline_summary):,}\")\n",
    "print(baseline_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Prophet per route (optional)\n",
    "\n",
    "Fit an expanding-window Prophet model when available (N?100) and merge the forecasts for side-by-side evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_walkforward(route_df: pd.DataFrame, min_history: int = 100, refit_every: int = 30) -> pd.Series:\n",
    "    \"\"\"Return expanding-window Prophet forecasts aligned to the route's index.\"\"\"\n",
    "    work = route_df.sort_values(\"date\").copy()\n",
    "    work = work.reset_index().rename(columns={\"index\": \"orig_index\"})\n",
    "    preds = np.full(len(work), np.nan, dtype=float)\n",
    "    if (not HAVE_PROPHET) or len(work) < min_history:\n",
    "        return pd.Series(preds, index=work[\"orig_index\"])\n",
    "    model = None\n",
    "    fitted_until = None\n",
    "    for i in range(len(work)):\n",
    "        if i < min_history:\n",
    "            continue\n",
    "        need_refit = (model is None) or (fitted_until is None) or ((i - fitted_until) >= refit_every)\n",
    "        if need_refit:\n",
    "            train = work.iloc[:i][[\"date\", \"price\"]].rename(columns={\"date\": \"ds\", \"price\": \"y\"})\n",
    "            try:\n",
    "                model = Prophet(\n",
    "                    growth=\"flat\",\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False,\n",
    "                    yearly_seasonality=False,\n",
    "                )\n",
    "                model.fit(train)\n",
    "                fitted_until = i\n",
    "            except Exception as exc:  # pragma: no cover - depends on cmdstan availability\n",
    "                warnings.warn(f\"Prophet fit failed for route {route_df['route'].iloc[0]}: {exc}\")\n",
    "                return pd.Series(preds, index=work[\"orig_index\"])\n",
    "        future = pd.DataFrame({\"ds\": [work.loc[i, \"date\"]]})\n",
    "        try:\n",
    "            preds[i] = model.predict(future)[\"yhat\"].iloc[0]\n",
    "        except Exception as exc:  # pragma: no cover - depends on model stability\n",
    "            warnings.warn(f\"Prophet predict failed for route {route_df['route'].iloc[0]}: {exc}\")\n",
    "            break\n",
    "    return pd.Series(preds, index=work[\"orig_index\"])\n",
    "\n",
    "if \"fcst_prophet\" not in ts_features.columns:\n",
    "    ts_features[\"fcst_prophet\"] = np.nan\n",
    "\n",
    "if HAVE_PROPHET:\n",
    "    for route, idx in ts_features.groupby(\"route\").groups.items():\n",
    "        route_slice = ts_features.loc[idx, [\"route\", \"date\", \"price\"]]\n",
    "        preds = prophet_walkforward(route_slice, min_history=100, refit_every=30)\n",
    "        ts_features.loc[preds.index, \"fcst_prophet\"] = preds.values\n",
    "else:\n",
    "    print(\"Prophet unavailable in this environment; skipping advanced model.\")\n",
    "\n",
    "if \"fcst_prophet\" in bt_with_features.columns:\n",
    "    bt_with_features = bt_with_features.drop(columns=[\"fcst_prophet\"])\n",
    "\n",
    "bt_with_features = bt_with_features.merge(\n",
    "    ts_features[[\"route\", \"date\", \"fcst_prophet\"]],\n",
    "    on=[\"route\", \"date\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "if HAVE_PROPHET and ts_features[\"fcst_prophet\"].notna().any():\n",
    "    bt_prophet = bt_with_features.dropna(subset=[\"fcst_prophet\"])\n",
    "    prophet_summary = kpis_by_route(bt_prophet, pred_col=\"fcst_prophet\")\n",
    "    prophet_summary = prophet_summary.rename(\n",
    "        columns={\n",
    "            \"mae\": \"mae_prophet\",\n",
    "            \"mape\": \"mape_prophet\",\n",
    "            \"n_obs\": \"n_obs_prophet\",\n",
    "            \"start_date\": \"start_date_prophet\",\n",
    "            \"end_date\": \"end_date_prophet\",\n",
    "        }\n",
    "    )\n",
    "    print(f\"Prophet KPI routes: {len(prophet_summary):,}\")\n",
    "    print(prophet_summary.head())\n",
    "else:\n",
    "    prophet_summary = pd.DataFrame()\n",
    "    print(\"No Prophet KPIs generated (insufficient data or library missing).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Save outputs for Power BI\n",
    "\n",
    "Combine baseline + Prophet metrics, export detail/summary CSVs, and preview the saved tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_cols = [\"route\", \"date\", \"actual\", \"fcst_baseline\", \"fcst_prophet\", \"median_30d\"]\n",
    "forecast_detail = bt_with_features.loc[:, detail_cols].sort_values([\"route\", \"date\"])\n",
    "forecast_detail.to_csv(OUT_DETAIL, index=False)\n",
    "\n",
    "summary_export = baseline_summary.copy()\n",
    "if prophet_summary.empty:\n",
    "    summary_export[\"mae_prophet\"] = np.nan\n",
    "    summary_export[\"mape_prophet\"] = np.nan\n",
    "else:\n",
    "    summary_export = summary_export.merge(\n",
    "        prophet_summary[[\"route\", \"mae_prophet\", \"mape_prophet\"]],\n",
    "        on=\"route\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "summary_export = summary_export.sort_values(\"mape_baseline\").reset_index(drop=True)\n",
    "summary_export.to_csv(OUT_SUM, index=False)\n",
    "\n",
    "print(f\"Detail rows saved: {len(forecast_detail):,} -> {OUT_DETAIL}\")\n",
    "print(forecast_detail.head())\n",
    "print(\"\n",
    "Summary snapshot:\")\n",
    "print(summary_export.head())\n",
    "print(f\"Summary rows saved: {len(summary_export):,} -> {OUT_SUM}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}